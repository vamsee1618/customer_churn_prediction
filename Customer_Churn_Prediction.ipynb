{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Venmo Customer Churn\n",
        "\n"
      ],
      "metadata": {
        "id": "tNtyxRxYZpSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the Spark and Loading the Dataset"
      ],
      "metadata": {
        "id": "Fr8o8WrdZ7aS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XwjDePbo6rs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cukI4W45wcin"
      },
      "outputs": [],
      "source": [
        "# Run below commands\n",
        "!echo \"setup Colab for PySpark $PYSPARK and Spark NLP $SPARKNLP\"\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install spark-nlp==4.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H76Dh23b6zlC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y56wjea7iwa"
      },
      "outputs": [],
      "source": [
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (col,datediff,when, lit,\n",
        "                                   regexp_replace,max,min, year,concat,month,lag,\n",
        "                                   coalesce,array_contains,length,udf,size,split,\n",
        "                                   explode, arrays_zip, mean, collect_list,concat_ws,avg)\n",
        "\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "#Spark ML and SQL\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
        "from pyspark.sql.functions import col\n",
        "#Spark NLP\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import RegexRule\n",
        "from sparknlp.base import DocumentAssembler, Finisher\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import ArrayType, IntegerType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7FFMcGz8lBk"
      },
      "outputs": [],
      "source": [
        "spark = sparknlp.start()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "# spark.conf.set()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-wnvU6B44kN"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HzwVcEY5ACb"
      },
      "outputs": [],
      "source": [
        "venmo = spark.read.parquet(\"drive/Shareddrives/Venmo_Project/VenmoSample.snappy.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTg_MNiCpvnr"
      },
      "source": [
        "### Creation of Churn Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeVwlJXK6xYQ"
      },
      "outputs": [],
      "source": [
        "venmo = venmo.withColumn(\"year_month\", concat(year(col(\"datetime\")).cast(StringType() ), lit(\"-\"),\n",
        "                                              when(month(col(\"datetime\")) <10,\n",
        "                                                   concat(lit(\"0\"), month(col(\"datetime\")).cast(StringType()))).otherwise(month(col(\"datetime\")).cast(StringType())) ))\n",
        "\n",
        "window_y_var = Window.partitionBy(\"user1\")\n",
        "\n",
        "venmo = venmo.withColumn(\"max_year_month\", max(col(\"year_month\")).over(window_y_var)).withColumn(\"min_year_month\", min(col(\"year_month\")).over(window_y_var))\n",
        "\n",
        "venmo = venmo.filter(col(\"min_year_month\") < \"2014-06\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bGP_P0r8ioY"
      },
      "outputs": [],
      "source": [
        "venmo = venmo.withColumn(\"churn\", when(col(\"max_year_month\") < \"2014-06\", 1).otherwise(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSbCDqCTw7Qr"
      },
      "outputs": [],
      "source": [
        "venmo = venmo.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwkHWJ7QoDr6"
      },
      "source": [
        "### Text Data Preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6ijxTlxvhVI"
      },
      "outputs": [],
      "source": [
        "emoji_pattern = u'[^\\U0001F300-\\U0001F64F\\U0001F680-\\U0001F6FF\\u2600-\\u26FF\\u2700-\\u27BF]'\n",
        "venmo = venmo.withColumn(\"emoji_only\",regexp_replace(col('description'), emoji_pattern, ''))\n",
        "punc_pattern = r'[^,|\\.|&|\\\\|\\||-|_|!]'\n",
        "venmo = venmo.withColumn(\"punctuations\", regexp_replace(\"description\", punc_pattern, \"\"))\n",
        "textonly_pattern = r'[^\\w\\s]|_'\n",
        "venmo = venmo.withColumn(\"text_only\",regexp_replace(col('description'), textonly_pattern, ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ND5WZfC5FMd"
      },
      "outputs": [],
      "source": [
        "venmo = venmo.withColumn(\"text_only\", when(col(\"text_only\") == \" \", None).otherwise(col(\"text_only\")))\n",
        "venmo = venmo.withColumn(\"punctuations\", when(col(\"punctuations\") == \"\", None).otherwise(col(\"punctuations\")))\n",
        "venmo = venmo.withColumn(\"emoji_only\", when(col(\"emoji_only\") == \"\", None).otherwise(col(\"emoji_only\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdH8yth7CMxV"
      },
      "outputs": [],
      "source": [
        "venmo = venmo.withColumn(\"characteronly_length\", length(\"text_only\"))\n",
        "venmo = venmo.withColumn(\"punctuations_length\", length(\"punctuations\"))\n",
        "venmo = venmo.withColumn(\"emoji_length\", length(\"emoji_only\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An_3ZsMko5QO"
      },
      "outputs": [],
      "source": [
        "### User Level Aggregation\n",
        "venmo_user = venmo.groupBy(\"user1\") \\\n",
        "              .agg(avg(\"churn\").alias(\"churn\"), \\\n",
        "                   concat_ws(\" \",collect_list(\"text_only\")).alias(\"text_only\"))\n",
        "venmo_user = venmo_user.filter(col('text_only').isNotNull() | (col('text_only') != ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bm7icG8vd44"
      },
      "outputs": [],
      "source": [
        "### User Level Aggregation\n",
        "venmo_user = venmo.groupBy(\"user1\") \\\n",
        "              .agg(avg(\"churn\").alias(\"churn\"), \\\n",
        "                   min(\"datetime\").alias(\"customer_join_Date\"),\\\n",
        "                   avg(\"characteronly_length\").alias(\"avg_characteronly_length\"), \\\n",
        "                   avg(\"punctuations_length\").alias(\"avg_punctuations_length\"), \\\n",
        "                   avg(\"emoji_length\").alias(\"avg_emoji_length\"), \\\n",
        "                   concat_ws(\" \",collect_list(\"punctuations\")).alias(\"punctuations\"),\\\n",
        "                   concat_ws(\" \",collect_list(\"emoji_only\")).alias(\"emoji_only\"),\\\n",
        "                   concat_ws(\" \", collect_list(\"text_only\")).alias(\"text_only\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descriptive Statistics"
      ],
      "metadata": {
        "id": "_Mj2n45FarZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha1BYa95y56D"
      },
      "outputs": [],
      "source": [
        "venmo_user.groupBy('churn').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cs_8U6E07Bp"
      },
      "outputs": [],
      "source": [
        "venmo_user.withColumn('join_year', year('customer_join_Date')) \\\n",
        "               .groupBy('join_year', 'churn') \\\n",
        "               .count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZtSYePS1rad"
      },
      "outputs": [],
      "source": [
        "venmo_user.groupBy('churn').agg(avg('avg_characteronly_length').alias('character_length'),\\\n",
        "                                avg('avg_punctuations_length').alias('punctuation_length'),\\\n",
        "                                avg('avg_emoji_length').alias('emoji_length'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Development"
      ],
      "metadata": {
        "id": "4WrADPmsa60m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x2hz7Ee4lOb"
      },
      "outputs": [],
      "source": [
        "# Perform stratified sampling\n",
        "venmo_user =  venmo_user.fillna(0)\n",
        "venmo_train = venmo_user.sampleBy('churn', fractions={0: 0.7, 1: 0.7}, seed=10)\n",
        "venmo_test = venmo_user.subtract(venmo_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5Xbaxp07KXh"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"avg_characteronly_length\",\"avg_punctuations_length\",\"avg_emoji_length\"], outputCol=\"features\")\n",
        "training_data = assembler.transform(venmo_train)\n",
        "testing_data = assembler.transform(venmo_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32lWpmb7_Cgo"
      },
      "outputs": [],
      "source": [
        "training_data.groupBy('churn').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe-KdjdGLrzR"
      },
      "outputs": [],
      "source": [
        "testing_data.groupBy('churn').count()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression with Count Variables"
      ],
      "metadata": {
        "id": "B6vDezqybhJ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4TZiBTm7jjQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "glr = GeneralizedLinearRegression(family=\"binomial\", maxIter=100, featuresCol='features',labelCol='churn')\n",
        "\n",
        "# Fit the model\n",
        "model = glr.fit(training_data)\n",
        "\n",
        "# Summarize the model over the training set and print out some metrics\n",
        "summary = model.summary\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHNG1ETfLxiO"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction_label\", labelCol=\"churn\")\n",
        "predictions = model.transform(training_data)\n",
        "threshold = 0.5\n",
        "predictions = predictions.withColumn('prediction_label', when(predictions['prediction'] >= threshold, 1).otherwise(0))\n",
        "predictions = predictions.withColumn('prediction_label', col('prediction_label').cast('double'))\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print(\"Train AUC: {:.4f}\".format(auc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhTIzDPYL6AK"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(testing_data)\n",
        "threshold = 0.5\n",
        "predictions = predictions.withColumn('prediction_label', when(predictions['prediction'] >= threshold, 1).otherwise(0))\n",
        "predictions = predictions.withColumn('prediction_label', col('prediction_label').cast('double'))\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print(\"Test AUC: {:.4f}\".format(auc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj7ehM4pPjJN"
      },
      "outputs": [],
      "source": [
        "predictions.crosstab('churn', 'prediction_label').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxHxMDu2UwjM"
      },
      "outputs": [],
      "source": [
        "print(\"Precision: \",(988/(988+512)))\n",
        "print(\"Recall: \",(988/(988+7298)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LIWC Variables Inclusion"
      ],
      "metadata": {
        "id": "dNVoaOi8bovL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uGCDQuoVoG1"
      },
      "outputs": [],
      "source": [
        "venmo_liwc = spark.read.csv(\"drive/Shareddrives/Venmo_Project/data/liwc_results.csv\", header=True, inferSchema=True)\n",
        "venmo_train = venmo_train.join(venmo_liwc, venmo_train.user1 == venmo_liwc.user1, \"left\")\n",
        "venmo_train = venmo_train.fillna(0)\n",
        "venmo_test = venmo_test.join(venmo_liwc, venmo_test.user1 == venmo_liwc.user1, \"left\")\n",
        "venmo_test = venmo_test.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V3KXpu-Xgs2"
      },
      "outputs": [],
      "source": [
        "features_list = ['Dic','avg_characteronly_length','avg_punctuations_length','avg_emoji_length','WC','BigWords'\n",
        " ,'pronoun','ppron','i','we','you','shehe','they','ipron','det','article','number','prep','auxverb','adverb','conj','negate', 'verb', 'adj', 'quantity', 'Drives',\n",
        " 'affiliation','achieve','power','Cognition','allnone','cogproc','insight','cause','discrep','tentat','certitude','differ','memory','Affect',\n",
        " 'tone_pos','tone_neg','emotion','emo_pos','emo_neg','emo_anx','emo_anger','emo_sad','swear','Social','socbehav','prosocial','polite','conflict','moral',\n",
        " 'comm','socrefs','family','friend','Culture','politic','ethnicity','tech','Lifestyle','leisure','home','work','money','relig','Physical','health','illness',\n",
        " 'wellness','mental','substances','sexual','food','death','need','want','acquire','lack','fulfill','fatigue','reward','risk','curiosity','allure',\n",
        " 'Perception', 'attention','motion', 'space', 'visual', 'auditory', 'feeling', 'time', 'focuspast', 'focuspresent', 'focusfuture', 'Conversation', 'netspeak', 'assent', 'nonflu', 'filler']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0LwXk7WXdnU"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(inputCols=features_list, outputCol=\"features\")\n",
        "training_data = assembler.transform(venmo_train)\n",
        "testing_data = assembler.transform(venmo_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSMJyRPE7K6t"
      },
      "outputs": [],
      "source": [
        "# IMPORT\n",
        "import numpy as np\n",
        "from numpy import allclose\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "\n",
        "# BUILD THE MODEL\n",
        "rf = RandomForestClassifier(numTrees=1000, maxDepth=5, labelCol=\"churn\", seed=42,subsamplingRate=0.4)\n",
        "model = rf.fit(training_data)\n",
        "\n",
        "# FEATURE IMPORTANCES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBetcXYtBmWB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Filter out zero importance features\n",
        "non_zero_indices = list(model.featureImportances.indices)\n",
        "non_zero_importances = list(model.featureImportances.values)\n",
        "non_zero_features = [features_list[index] for index in non_zero_indices]\n",
        "\n",
        "# Sort the features by importance in decreasing order\n",
        "sorted_indices = np.argsort(non_zero_importances)\n",
        "non_zero_indices = [non_zero_indices[i] for i in sorted_indices]\n",
        "non_zero_importances = [non_zero_importances[i] for i in sorted_indices]\n",
        "non_zero_features = [non_zero_features[i] for i in sorted_indices]\n",
        "\n",
        "# Increase the size of the plot\n",
        "plt.figure(figsize=(10, 24))\n",
        "\n",
        "# Creating the bar chart\n",
        "plt.barh(np.arange(len(non_zero_features)), non_zero_importances, align='center')\n",
        "plt.yticks(np.arange(len(non_zero_features)), non_zero_features)\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.title('Feature Importances')\n",
        "\n",
        "# Displaying the chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKpIUyIucjEZ"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction_label\", labelCol=\"churn\")\n",
        "predictions = model.transform(training_data)\n",
        "threshold = 0.5\n",
        "predictions = predictions.withColumn('prediction_label', when(predictions['prediction'] >= threshold, 1).otherwise(0))\n",
        "predictions = predictions.withColumn('prediction_label', col('prediction_label').cast('double'))\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print(\"Train AUC: {:.4f}\".format(auc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oMcydEDco9y"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(testing_data)\n",
        "threshold = 0.5\n",
        "predictions = predictions.withColumn('prediction_label', when(predictions['prediction'] >= threshold, 1).otherwise(0))\n",
        "predictions = predictions.withColumn('prediction_label', col('prediction_label').cast('double'))\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print(\"Test AUC: {:.4f}\".format(auc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGN_tBn4wJ6K"
      },
      "outputs": [],
      "source": [
        "predictions.crosstab('churn', 'prediction_label').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-xZk5RNwRpt"
      },
      "outputs": [],
      "source": [
        "print(\"Precision: \",(4981/(4981+592)))\n",
        "print(\"Recall: \",(4981/(4981+3305)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Development in Pandas, as sample being smaller size"
      ],
      "metadata": {
        "id": "bWCYH3PWcJ2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
        "import shap\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sqmeIEOVchQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = venmo_train.toPandas()\n",
        "df_test = venmo_test.toPandas()"
      ],
      "metadata": {
        "id": "ErVk_qeycWzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=1000, criterion='log_loss', max_depth=10, min_samples_split=50, min_samples_leaf=25,\n",
        "                            max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=True, n_jobs=-1, random_state=1000,\n",
        "                            verbose=3, warm_start=False)\n",
        "rf.fit(df_train[features_list], df_train['churn'])"
      ],
      "metadata": {
        "id": "JCHhDx1BckQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf.oob_score_"
      ],
      "metadata": {
        "id": "ee5Y8G8Zc2KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rf.predict(df_test[features_list])\n",
        "precision = precision_score(df_test['churn'], y_pred)\n",
        "recall = recall_score(df_test['churn'], y_pred)\n",
        "print(precision)\n",
        "print(recall)"
      ],
      "metadata": {
        "id": "Xhyk2iYgc23w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(df_test['churn'], y_pred)"
      ],
      "metadata": {
        "id": "Qhtdsnl6c59E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "feature_importance = rf.feature_importances_\n",
        "sorted_indices = np.argsort(feature_importance)[::-1]\n",
        "sorted_importance = feature_importance[sorted_indices]\n",
        "sorted_features = features_list\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(range(len(sorted_importance)), sorted_importance, tick_label=sorted_features)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Feature Importances')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OCzVC_dIc8st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# explainer = shap.TreeExplainer(rf)\n",
        "# shap_values = explainer.shap_values(df_test[features_list])"
      ],
      "metadata": {
        "id": "yCjrQD_kdDRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shap.summary_plot(shap_values, df_test[features_list])"
      ],
      "metadata": {
        "id": "PxlHSsNGdFpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shap.summary_plot(shap_values[1], df_test[features_list])"
      ],
      "metadata": {
        "id": "gHAPi4yGdH0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shap.summary_plot(shap_values[0], df_test[features_list])"
      ],
      "metadata": {
        "id": "uT2viD82dLIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embdeddings and Model Development"
      ],
      "metadata": {
        "id": "CnRUG8YhdSfm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8Rz0dBFzxwZ"
      },
      "outputs": [],
      "source": [
        "# Spark NLP Pipeline\n",
        "### Text Tokens\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text_only\")\n",
        "\n",
        "documentNormalizer = DocumentNormalizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"normalizedDocument\") \\\n",
        "    .setLowercase(True)\n",
        "\n",
        "sentence_detector = SentenceDetector() \\\n",
        "    .setInputCols([\"normalizedDocument\"]) \\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"token_lemma\")\n",
        "\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"token_lemma\"]) \\\n",
        "    .setOutputCols([\"text_tokens\"]) \\\n",
        "    .setIncludeMetadata(False) \\\n",
        "    .setOutputAsArray(True)\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    documentNormalizer,\n",
        "    sentence_detector,\n",
        "    tokenizer,\n",
        "    lemmatizer,\n",
        "    finisher\n",
        "])\n",
        "\n",
        "### Emoji Tokens\n",
        "document_assembler_emoji = DocumentAssembler() \\\n",
        "    .setInputCol(\"emoji_only\")\n",
        "\n",
        "sentence_detector_emoji = SentenceDetector() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer_emoji = Tokenizer() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "finisher_emoji = Finisher() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCols([\"emoji_tokens\"]) \\\n",
        "    .setIncludeMetadata(False) \\\n",
        "    .setOutputAsArray(True)\n",
        "\n",
        "nlpPipeline_emoji = Pipeline(stages=[\n",
        "    document_assembler_emoji,\n",
        "    sentence_detector_emoji,\n",
        "    tokenizer_emoji,\n",
        "    finisher_emoji\n",
        "])\n",
        "\n",
        "### punctuations\n",
        "document_assembler_punc = DocumentAssembler() \\\n",
        "    .setInputCol(\"punctuations\")\n",
        "\n",
        "sentence_detector_punc = SentenceDetector() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer_punc= Tokenizer() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "finisher_punc = Finisher() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCols([\"punctuation_tokens\"]) \\\n",
        "    .setIncludeMetadata(False) \\\n",
        "    .setOutputAsArray(True)\n",
        "\n",
        "nlpPipeline_punc = Pipeline(stages=[\n",
        "    document_assembler_punc,\n",
        "    sentence_detector_punc,\n",
        "    tokenizer_punc,\n",
        "    finisher_punc\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEFgn0fQ0x8l"
      },
      "outputs": [],
      "source": [
        "venmo_user = nlpPipeline.fit(venmo_user).transform(venmo_user)\n",
        "venmo_user = nlpPipeline_emoji.fit(venmo_user).transform(venmo_user)\n",
        "venmo_user = nlpPipeline_punc.fit(venmo_user).transform(venmo_user)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stratified sampling\n",
        "venmo_user =  venmo_user.fillna(0)\n",
        "venmo_train = venmo_user.sampleBy('churn', fractions={0: 0.7, 1: 0.7}, seed=10)\n",
        "venmo_test = venmo_user.subtract(venmo_train)"
      ],
      "metadata": {
        "id": "LlUkxaN9k9_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt14G_qX7NgH"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXt5vwGwXtl0"
      },
      "outputs": [],
      "source": [
        "hashingTF = HashingTF(inputCol=\"text_tokens\", outputCol=\"countfeatures\")\n",
        "\n",
        "idf = IDF(inputCol=\"countfeatures\", outputCol=\"tf_idf_features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "\n",
        "nlp_pipeline_tf = Pipeline(\n",
        "    stages=[\n",
        "            hashingTF,\n",
        "            idf\n",
        "            ])\n",
        "\n",
        "nlp_model_tf = nlp_pipeline_tf.fit(venmo_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r2TpmKWYzMb"
      },
      "outputs": [],
      "source": [
        "venmo_train = nlp_model_tf.transform(venmo_train)\n",
        "venmo_test = nlp_model_tf.transform(venmo_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count Features and Modelling"
      ],
      "metadata": {
        "id": "Dbf5wgNwd05r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfBpJmQqX5Ut"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"countfeatures\"], outputCol=\"features\")\n",
        "training_data = assembler.transform(venmo_train)\n",
        "\n",
        "# rf = RandomForestClassifier(numTrees=100, maxDepth=5, labelCol=\"churn\", seed=42,subsamplingRate=0.4)\n",
        "# model = rf.fit(training_data)\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"churn\")\n",
        "model = lr.fit(training_data)\n",
        "\n",
        "# make predictions on the testing data\n",
        "predictions = model.transform(training_data)\n",
        "predictions.show(1,0)\n",
        "\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"churn\")\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print(\"Train AUC: {:.4f}\".format(auc))\n",
        "\n",
        "# create a feature vector from the \"name\" column in the testing data\n",
        "testing_data = assembler.transform(venmo_test)\n",
        "\n",
        "# make predictions on the testing data\n",
        "predictions = model.transform(testing_data)\n",
        "predictions.show(1,0)\n",
        "\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"churn\")\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print(\"Test AUC: {:.4f}\".format(auc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPU433aulKQ8"
      },
      "source": [
        "### TF-IDF Features and Model Development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdG7m1qIk704"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"tf_idf_features\"], outputCol=\"features\")\n",
        "training_data = assembler.transform(venmo_train)\n",
        "\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"churn\")\n",
        "model = lr.fit(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQUr_os6I8g0"
      },
      "outputs": [],
      "source": [
        "# make predictions on the testing data\n",
        "predictions = model.transform(training_data)\n",
        "predictions.show(1,0)\n",
        "\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"churn\")\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print(\"Train AUC: {:.4f}\".format(auc))\n",
        "\n",
        "# create a feature vector from the \"name\" column in the testing data\n",
        "testing_data = assembler.transform(venmo_test)\n",
        "\n",
        "# make predictions on the testing data\n",
        "predictions = model.transform(testing_data)\n",
        "predictions.show(1,0)\n",
        "\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"churn\")\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print(\"Test AUC: {:.4f}\".format(auc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgLkHLSwJZS4"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"churn\")\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "print(\"AUC: {:.4f}\".format(auc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QCCa_ORqKmm"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "\n",
        "# Define the input data\n",
        "data = [(\"example\",),\n",
        "        (\"yet\",),\n",
        "        (\"example another yet example\",)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"text_only\"])\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "df = nlpPipeline.fit(df).transform(df)\n",
        "\n",
        "df = nlp_pipeline_tf.fit(df).transform(df)\n",
        "df.show(3,0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CIJIZmNuhpvd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}